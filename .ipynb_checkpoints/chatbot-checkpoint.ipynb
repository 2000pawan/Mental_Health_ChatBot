{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "570156f5-03a9-4b36-8765-57fc1e7f0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b592101c-dab5-4c76-a185-8b58aed71f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "df=pd.read_csv(\"data_to_be_cleansed.csv\")\n",
    "#Remove redundant columns\n",
    "df=df.iloc[::,1:]\n",
    "#Drop rows with missing 'text' values\n",
    "df=df.dropna(subset=['text'])\n",
    "#Remove Duplicates\n",
    "df=df.drop_duplicates(subset=['text','title'])\n",
    "# Combine 'text' and 'title' columns\n",
    "df['input'] = df['text'] + \" \" + df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1388d07-317e-4b27-9141-7ef0d60f060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to 'text' and 'title' columns\n",
    "df['input_text'] = df['input'].apply(preprocess_text)\n",
    "# Drop the original 'text' and 'title' columns if not needed\n",
    "df.drop(columns=['text', 'title', 'input'], inplace=True)\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e84e5c9-ce8b-4afb-91d5-654b9992b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df1=pd.read_csv('clean.csv')\n",
    "# Initialize TF-IDF vectorizer\n",
    "vz=TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "# Extract features from processed text\n",
    "X=vz.fit_transform(df1['input_text'])\n",
    "# Target column\n",
    "y=df1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5920bfb-fed5-4a12-b030-b5695fbcfa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 3\n",
      "Class Probabilities: [0.21427585 0.19659756 0.1697841  0.22710385 0.19223864]\n"
     ]
    }
   ],
   "source": [
    "# Example user input\n",
    "new_text = [input()]\n",
    " # Replace with your text\n",
    "\n",
    "# Preprocess the input (vectorize using the same TF-IDF vectorizer)\n",
    "new_text_vectorized = vz.transform(new_text)\n",
    "\n",
    "# Predict the class\n",
    "predicted_class = model.predict(new_text_vectorized)\n",
    "\n",
    "# Get the probability of each class\n",
    "predicted_probabilities = model.predict_proba(new_text_vectorized)\n",
    "\n",
    "# Display the results\n",
    "print(\"Predicted Class:\", predicted_class[0])\n",
    "print(\"Class Probabilities:\", predicted_probabilities[0])\n",
    "dic={0:'Stress',1:'Depression',2:'Bipolar disorder',3:'Personality disorder',4:'Anxiety'}\n",
    "result=dic[predicted_class[0]]\n",
    "new_text=f'I am suffering from {result}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc5cd7ba-21db-4422-b68d-1295af938390",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump=joblib.dump(model,'model.pkl')\n",
    "dump2=joblib.dump(vz,'vz.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d084d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
